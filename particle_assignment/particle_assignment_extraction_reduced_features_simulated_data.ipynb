{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import keras\n",
    "\n",
    "import keras.backend as K\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from keras import optimizers, metrics\n",
    "from keras.layers import Dense, LayerNormalization, BatchNormalization, Dropout, GaussianNoise\n",
    "from keras.models import load_model\n",
    "from tools.constants import column_labels_mass_reco\n",
    "from tools.constants import column_labels_particle_assignment as column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modified from source: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "# Used to serve data from files to the neural network.\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, n_features=73, shuffle=True, data_path=\"\", scaler = \"scaler_params/reduced_variables_scaler.csv.csv\", write_to_file = False, standardize = True):\n",
    "        'Initialization'\n",
    "        self.n_features = n_features\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.write_to_file = write_to_file\n",
    "        self.standardize = standardize\n",
    "        self.data_path = data_path\n",
    "        self.used_features = [\n",
    "                            \"event id\",\"main lepton px\",\"main lepton py\",\"main lepton pz\",\"main lepton E\",\\\n",
    "                            \"antitop lepton px\",\"antitop lepton py\",\"antitop lepton pz\",\"antitop lepton E\",\\\n",
    "                            \"hadr tau px\",\"hadr tau py\",\"hadr tau pz\", \\\n",
    "                            \"HT\", \"lep tau decay type\",\"hadr tau and main lep delta R\",\"hadr tau and antitop lep delta R\", \\\n",
    "                            \"main lepton mass\",\"antitop lepton mass\",\"visible main px\",\"visible main py\",\"visible main pz\",\"met x\", \"met y\",\"num permutations\", \\\n",
    "                            \"true main px\",\"true main py\",\"true main pz\",\"true main E\"]\n",
    "        with open(scaler) as f:\n",
    "            scaler_params = np.loadtxt(f, delimiter=\",\")\n",
    "            self.scaler = scaler_params\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y, weights = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y, weights\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((1,self.n_features),dtype=float)\n",
    "        y = np.zeros((1,5),dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample      \n",
    "            X = np.concatenate((X,np.load(self.data_path + str(int(ID)) + '.npy')),axis=0)\n",
    "\n",
    "            # Store class\n",
    "            y = np.concatenate((y,self.labels[str(int(ID))]),axis=0)\n",
    "\n",
    "        X = X[1:,:]\n",
    "        X = X[:,[column_names.index(var_name) for var_name in self.used_features]]  \n",
    "        y = y[1:,:]\n",
    "\n",
    "        weights = np.reciprocal(X[:,-1])*200\n",
    "\n",
    "        if self.write_to_file:\n",
    "            X = np.concatenate((X[:,:-5],X[:,-4:]), axis = 1)   # Exclude weights.\n",
    "        else:\n",
    "            X = X[:,1:-5]\n",
    "            if self.standardize:\n",
    "                X = (X-self.scaler[0])/self.scaler[1]     # Standardize\n",
    "\n",
    "        return X, y, weights\n",
    "        \n",
    "    def get_all(self):\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for i in range(self.__len__()):\n",
    "            X_y = self.__getitem__(i)\n",
    "            X += X_y[0].tolist()\n",
    "            y += X_y[1].tolist()\n",
    "\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loss function. \"\"\"\n",
    "# source: https://stackoverflow.com/questions/46009619/keras-weighted-binary-crossentropy\n",
    "def create_weighted_binary_crossentropy(ones_weights,zeros_weights):\n",
    "\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "\n",
    "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
    "        weight_vector = y_true * ones_weights + (1. - y_true) * zeros_weights\n",
    "        weighted_b_ce = weight_vector * b_ce\n",
    "\n",
    "        return K.mean(weighted_b_ce)\n",
    "\n",
    "    return weighted_binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Metrics \"\"\"\n",
    "# source: https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras\n",
    "def custom_f1(y_true, y_pred):    \n",
    "    def recall_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        \n",
    "        recall = TP / (Positives+K.epsilon())    \n",
    "        return recall \n",
    "    \n",
    "    \n",
    "    def precision_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    \n",
    "        precision = TP / (Pred_Positives+K.epsilon())\n",
    "        return precision \n",
    "    \n",
    "    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n",
    "    \n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# source: https://stackoverflow.com/questions/39895742/matthews-correlation-coefficient-with-keras\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "      matthews_correlation,\n",
    "      custom_f1,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model.\n",
    "model = load_model(\"models/particle_assignment_model_real\", custom_objects={'custom_f1':custom_f1, 'matthews_correlation':matthews_correlation, 'weighted_binary_crossentropy':create_weighted_binary_crossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load narrow selection ttH, ttZ data from files. \"\"\"\n",
    "\n",
    "with open(\"data/particle_assignment_training_data/labels_dict_ttH.csv\") as f:\n",
    "    lines_ttH = f.readlines()\n",
    "with open(\"data/particle_assignment_training_data/labels_dict_ttZ.csv\") as f:\n",
    "    lines_ttZ = f.readlines()\n",
    "\n",
    "with open(\"data/particle_assignment_training_data/train_ids_ttH.csv\") as f:\n",
    "    train_ids_ttH = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_training_data/train_ids_ttZ.csv\") as f:\n",
    "    train_ids_ttZ = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "with open(\"data/particle_assignment_training_data/test_ids_ttH.csv\") as f:\n",
    "    test_ids_ttH = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_training_data/test_ids_ttZ.csv\") as f:\n",
    "    test_ids_ttZ = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "with open(\"data/particle_assignment_training_data/val_ids_ttH.csv\") as f:\n",
    "    val_ids_ttH = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_training_data/val_ids_ttZ.csv\") as f:\n",
    "    val_ids_ttZ = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "lines = np.concatenate((lines_ttH,lines_ttZ), axis = 0)\n",
    "\n",
    "labels_dict = {}\n",
    "\n",
    "for line in lines:\n",
    "    row = np.fromstring(line, dtype=float, sep=',')\n",
    "    key = str(int(row[0]))\n",
    "    value = (row[1:]).reshape((int(len(row[1:])/5),5))\n",
    "    labels_dict[key] = value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load wide selection ttH, ttZ data from files. \"\"\"\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/labels_dict_ttH.csv\") as f:\n",
    "    lines_ttH = f.readlines()\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/labels_dict_ttZ.csv\") as f:\n",
    "    lines_ttZ = f.readlines()\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/train_ids_ttH.csv\") as f:\n",
    "    train_ids_ttH = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/train_ids_ttZ.csv\") as f:\n",
    "    train_ids_ttZ = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/test_ids_ttH.csv\") as f:\n",
    "    test_ids_ttH = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/test_ids_ttZ.csv\") as f:\n",
    "    test_ids_ttZ = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/val_ids_ttH.csv\") as f:\n",
    "    val_ids_ttH = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/val_ids_ttZ.csv\") as f:\n",
    "    val_ids_ttZ = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "lines = np.concatenate((lines_ttH,lines_ttZ), axis = 0)\n",
    "\n",
    "labels_dict = {}\n",
    "\n",
    "for line in lines:\n",
    "    row = np.fromstring(line, dtype=float, sep=',')\n",
    "    key = str(int(row[0]))\n",
    "    value = (row[1:]).reshape((int(len(row[1:])/5),5))\n",
    "    labels_dict[key] = value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Same as for narrow selection, but this time the wide selection ttH and ttZ is used. \"\"\"\n",
    "\n",
    "ids_list = [train_ids_ttZ, val_ids_ttZ, test_ids_ttZ]\n",
    "file_name_endings = [\"train_ttZ\",\"val_ttZ\",\"test_ttZ\"]\n",
    "\n",
    "\n",
    "for ids,ending in zip(ids_list,file_name_endings): \n",
    "    predict_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=False, standardize=True, data_path = \"data/particle_assignment_data_to_be_processed_ttH_ttZ/data/\")\n",
    "    write_to_file_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=True, standardize=False, data_path = \"data/particle_assignment_data_to_be_processed_ttH_ttZ/data/\")\n",
    "\n",
    "    test_ones_diff = 0 \n",
    "    test_samples_count = 0\n",
    "\n",
    "    data = []\n",
    "    all_best_ys = []\n",
    "    all_best_possible_ys = []\n",
    "\n",
    "    y_pred_best = []\n",
    "    y_true_best = []\n",
    "    for i in range(len(predict_generator)):\n",
    "        X_test = predict_generator[i][0]\n",
    "        y_test = predict_generator[i][1]\n",
    "\n",
    "        X_to_file = write_to_file_generator[i][0]\n",
    "\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        X_y_preds = np.concatenate((X_to_file, y_test, preds),axis=1)\n",
    "        \n",
    "        ids = np.unique(X_to_file[:,0])\n",
    "\n",
    "        X_to_file = X_to_file[:,1:]\n",
    "\n",
    "        for id in ids:\n",
    "            X_y_preds_all_combinations = np.array([row[1:] for row in X_y_preds if row[0] == id])\n",
    "            X_all_combinations = X_y_preds_all_combinations[:,:X_to_file.shape[1]]\n",
    "            preds_all_combinations = X_y_preds_all_combinations[:,X_to_file.shape[1]+y_test.shape[1]:]\n",
    "\n",
    "            product = np.product(preds_all_combinations, axis=1)\n",
    "\n",
    "            best_pred = preds_all_combinations[np.argmax(product)]\n",
    "            best_X = X_all_combinations[np.argmax(product)]\n",
    "\n",
    "            data += [best_X.tolist() + best_pred.tolist()]\n",
    "\n",
    "    f = open(\"data/mass_reco/mass_reco_input_wide_selection_real_\" + ending + \".csv\", \"w\")\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(column_labels_mass_reco)\n",
    "    writer.writerows(data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load wide selection ttW, tt data from files. \"\"\"\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/labels_dict_ttW.csv\") as f:\n",
    "    lines_ttW = f.readlines()\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/labels_dict_tt.csv\") as f:\n",
    "    lines_tt = f.readlines()\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/train_ids_ttW.csv\") as f:\n",
    "    train_ids_ttW = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/train_ids_tt.csv\") as f:\n",
    "    train_ids_tt = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/test_ids_ttW.csv\") as f:\n",
    "    test_ids_ttW = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/test_ids_tt.csv\") as f:\n",
    "    test_ids_tt = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/val_ids_ttW.csv\") as f:\n",
    "    val_ids_ttW = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/val_ids_tt.csv\") as f:\n",
    "    val_ids_tt = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "lines = np.concatenate((lines_ttW,lines_tt), axis = 0)\n",
    "\n",
    "labels_dict = {}\n",
    "\n",
    "for line in lines:\n",
    "    row = np.fromstring(line, dtype=float, sep=',')\n",
    "    key = str(int(row[0]))\n",
    "    value = (row[1:]).reshape((int(len(row[1:])/5),5))\n",
    "    labels_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6197/3279970780.py:67: RuntimeWarning: divide by zero encountered in reciprocal\n",
      "  weights = np.reciprocal(X[:,-1])*200\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Same as for narrow selection, but this time the wide selection ttW and tt is used. \"\"\"\n",
    "\n",
    "ids_list = [train_ids_ttW, val_ids_ttW, test_ids_ttW, train_ids_tt, val_ids_tt, test_ids_tt]\n",
    "file_name_endings = [\"train_ttW\",\"val_ttW\",\"test_ttW\",\"train_tt\",\"val_tt\",\"test_tt\"]\n",
    "\n",
    "\n",
    "for ids,ending in zip(ids_list,file_name_endings): \n",
    "    predict_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=False, standardize=True, data_path=\"data/particle_assignment_data_to_be_processed_ttW_tt/data/\")\n",
    "    write_to_file_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=True, standardize=False, data_path=\"data/particle_assignment_data_to_be_processed_ttW_tt/data/\")\n",
    "\n",
    "    test_ones_diff = 0 \n",
    "    test_samples_count = 0\n",
    "\n",
    "    data = []\n",
    "    all_best_ys = []\n",
    "    all_best_possible_ys = []\n",
    "\n",
    "    y_pred_best = []\n",
    "    y_true_best = []\n",
    "    for i in range(len(predict_generator)):\n",
    "        X_test = predict_generator[i][0]\n",
    "        y_test = predict_generator[i][1]\n",
    "\n",
    "        X_to_file = write_to_file_generator[i][0]\n",
    "\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        X_y_preds = np.concatenate((X_to_file, y_test, preds),axis=1)\n",
    "        \n",
    "        ids = np.unique(X_to_file[:,0])\n",
    "\n",
    "        X_to_file = X_to_file[:,1:]\n",
    "\n",
    "        for id in ids:\n",
    "            X_y_preds_all_combinations = np.array([row[1:] for row in X_y_preds if row[0] == id])\n",
    "            X_all_combinations = X_y_preds_all_combinations[:,:X_to_file.shape[1]]\n",
    "            preds_all_combinations = X_y_preds_all_combinations[:,X_to_file.shape[1]+y_test.shape[1]:]\n",
    "\n",
    "            product = np.product(preds_all_combinations, axis=1)\n",
    "\n",
    "            best_pred = preds_all_combinations[np.argmax(product)]\n",
    "            best_X = X_all_combinations[np.argmax(product)]\n",
    "\n",
    "            data += [best_X.tolist() + best_pred.tolist()]\n",
    "\n",
    "    f = open(\"data/mass_reco/mass_reco_input_wide_selection_real_\" + ending + \".csv\", \"w\")\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(column_labels_mass_reco)\n",
    "    writer.writerows(data)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "faf4958e2dce26c676752487d7d4cf22fe757853dc487cc7f633ed9404dfd7e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('conda_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
