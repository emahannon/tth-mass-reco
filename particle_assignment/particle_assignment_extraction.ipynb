{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import keras\n",
    "\n",
    "import keras.backend as K\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from keras import optimizers, metrics\n",
    "from keras.layers import Dense, LayerNormalization, BatchNormalization, Dropout, GaussianNoise\n",
    "from keras.models import load_model\n",
    "from tools.constants import column_labels_mass_reco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modified from source: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "# Used to serve data from files to the neural network.\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, n_features=73+4, shuffle=True, data_path=\"\", scaler = \"scaler_params/particle_assignment_scaler_params.csv\", write_to_file = False, standardize = True):\n",
    "        'Initialization'\n",
    "        self.n_features = n_features\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.write_to_file = write_to_file\n",
    "        self.standardize = standardize\n",
    "        self.data_path = data_path\n",
    "        with open(scaler) as f:\n",
    "            scaler_params = np.loadtxt(f, delimiter=\",\")\n",
    "            self.scaler = scaler_params\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y, weights = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y, weights\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((1,self.n_features),dtype=float)\n",
    "        y = np.zeros((1,5),dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample      \n",
    "            X = np.concatenate((X,np.load(self.data_path +'/'+ str(int(ID)) + '.npy')),axis=0)\n",
    "\n",
    "            # Store class\n",
    "            y = np.concatenate((y,self.labels[str(int(ID))]),axis=0)\n",
    "\n",
    "        X = X[1:,:]\n",
    "        y = y[1:,:]\n",
    "\n",
    "        weights = np.reciprocal(X[:,-1])*200\n",
    "\n",
    "        if self.write_to_file:\n",
    "            X = np.concatenate((X[:,:-5],X[:,-4:]), axis = 1)   # Exclude weights.\n",
    "        else:\n",
    "            X = X[:,1:-5]\n",
    "            if self.standardize:\n",
    "                X = (X-self.scaler[0])/self.scaler[1]     # Standardize\n",
    "\n",
    "        return X, y, weights\n",
    "        \n",
    "    def get_all(self):\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for i in range(self.__len__()):\n",
    "            X_y = self.__getitem__(i)\n",
    "            X += X_y[0].tolist()\n",
    "            y += X_y[1].tolist()\n",
    "\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loss function. \"\"\"\n",
    "# source: https://stackoverflow.com/questions/46009619/keras-weighted-binary-crossentropy\n",
    "def create_weighted_binary_crossentropy(ones_weights,zeros_weights):\n",
    "\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "\n",
    "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
    "        weight_vector = y_true * ones_weights + (1. - y_true) * zeros_weights\n",
    "        weighted_b_ce = weight_vector * b_ce\n",
    "\n",
    "        return K.mean(weighted_b_ce)\n",
    "\n",
    "    return weighted_binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 13:09:00.621579: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-04 13:09:00.622486: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-04 13:09:00.625906: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Metrics \"\"\"\n",
    "# source: https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras\n",
    "def custom_f1(y_true, y_pred):    \n",
    "    def recall_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        \n",
    "        recall = TP / (Positives+K.epsilon())    \n",
    "        return recall \n",
    "    \n",
    "    \n",
    "    def precision_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    \n",
    "        precision = TP / (Pred_Positives+K.epsilon())\n",
    "        return precision \n",
    "    \n",
    "    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n",
    "    \n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# source: https://stackoverflow.com/questions/39895742/matthews-correlation-coefficient-with-keras\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "      matthews_correlation,\n",
    "      custom_f1,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model.\n",
    "model = load_model(\"models/particle_assignment_model\", custom_objects={'custom_f1':custom_f1, 'matthews_correlation':matthews_correlation, 'weighted_binary_crossentropy':create_weighted_binary_crossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load narrow selection ttH, ttZ data from files. \"\"\"\n",
    "\n",
    "with open(\"data/particle_assignment_training_data/labels_dict_ttH.csv\") as f:\n",
    "    lines_ttH = f.readlines()\n",
    "with open(\"data/particle_assignment_training_data/labels_dict_ttZ.csv\") as f:\n",
    "    lines_ttZ = f.readlines()\n",
    "\n",
    "with open(\"data/particle_assignment_training_data/train_ids_ttH.csv\") as f:\n",
    "    train_ids_ttH = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_training_data/train_ids_ttZ.csv\") as f:\n",
    "    train_ids_ttZ = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "with open(\"data/particle_assignment_training_data/test_ids_ttH.csv\") as f:\n",
    "    test_ids_ttH = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_training_data/test_ids_ttZ.csv\") as f:\n",
    "    test_ids_ttZ = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "with open(\"data/particle_assignment_training_data/val_ids_ttH.csv\") as f:\n",
    "    val_ids_ttH = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_training_data/val_ids_ttZ.csv\") as f:\n",
    "    val_ids_ttZ = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "lines = np.concatenate((lines_ttH,lines_ttZ), axis = 0)\n",
    "\n",
    "labels_dict = {}\n",
    "\n",
    "for line in lines:\n",
    "    row = np.fromstring(line, dtype=float, sep=',')\n",
    "    key = str(int(row[0]))\n",
    "    value = (row[1:]).reshape((int(len(row[1:])/5),5))\n",
    "    labels_dict[key] = value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32066115702479336\n",
      "0.30945821854912764\n"
     ]
    }
   ],
   "source": [
    "\"\"\" The neural network produces probabilitise of each of the five positions being correctly assigned in a permutation of an event.\n",
    "The permutation with the largest product of the respective probabilities is chosen.\n",
    "The data of the chosen permutation is then saved to a file to be used in the mass reconstruction \"\"\"\n",
    "\n",
    "# Narrow ttH, ttZ.\n",
    "\n",
    "#ids_list = [train_ids_ttH, val_ids_ttH, test_ids_ttH, train_ids_ttZ, val_ids_ttZ, test_ids_ttZ]\n",
    "#file_name_endings = [\"train_ttH\",\"val_ttH\",\"test_ttH\",\"train_ttZ\",\"val_ttZ\",\"test_ttZ\"]\n",
    "ids_list = [test_ids_ttH, test_ids_ttZ]\n",
    "file_name_endings = [\"test_ttH\",\"test_ttZ\"]\n",
    "\n",
    "for ids,ending in zip(ids_list,file_name_endings): \n",
    "    \"\"\" One generator for obtaining the probability vectors. Second generator to obtain the precise data we want to save to the file. \"\"\"\n",
    "    predict_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=False, standardize=True, data_path = \"data/particle_assignment_training_data/data\")\n",
    "    write_to_file_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=True, standardize=False, data_path = \"data/particle_assignment_training_data/data\")\n",
    "\n",
    "    test_ones_diff = 0 \n",
    "    test_samples_count = 0\n",
    "\n",
    "    data = []\n",
    "    all_best_ys = []\n",
    "    all_best_possible_ys = []\n",
    "\n",
    "    y_pred_best = []\n",
    "    y_true_best = []\n",
    "    best_matched = []\n",
    "\n",
    "    for i in range(len(predict_generator)):\n",
    "        X_test = predict_generator[i][0]\n",
    "        y_test = predict_generator[i][1]\n",
    "\n",
    "        X_to_file = write_to_file_generator[i][0]\n",
    "\n",
    "        \"\"\" Get probability vectors. \"\"\"\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        X_y_preds = np.concatenate((X_to_file, y_test, preds),axis=1)\n",
    "\n",
    "        ids = np.unique(X_to_file[:,0])\n",
    "\n",
    "        X_to_file = X_to_file[:,1:]\n",
    "\n",
    "        \"\"\" Each ID represents one event. \"\"\"\n",
    "        for id in ids:\n",
    "            X_y_preds_all_combinations = np.array([row[1:] for row in X_y_preds if row[0] == id])\n",
    "            X_all_combinations = X_y_preds_all_combinations[:,:X_to_file.shape[1]]\n",
    "            y_all_combinations = X_y_preds_all_combinations[:,X_to_file.shape[1]:X_to_file.shape[1]+y_test.shape[1]]\n",
    "            preds_all_combinations = X_y_preds_all_combinations[:,X_to_file.shape[1]+y_test.shape[1]:]\n",
    "\n",
    "            product = np.product(preds_all_combinations, axis=1)\n",
    "\n",
    "            \"\"\" Largest product yields the best (chosen) assignment. \"\"\"\n",
    "            best_pred = preds_all_combinations[np.argmax(product)]  \n",
    "            best_y = y_all_combinations[np.argmax(product)]\n",
    "            best_possible_y = y_all_combinations[np.argmax(np.sum(y_all_combinations, axis = 1))]\n",
    "\n",
    "            best_matched.append(np.sum(best_y) >= np.sum(best_possible_y))\n",
    "\n",
    "            all_best_ys.append(best_y)\n",
    "            all_best_possible_ys.append(best_possible_y)\n",
    "\n",
    "            best_X = X_all_combinations[np.argmax(product)]\n",
    "\n",
    "            \"\"\" Save data we will later write to file. \"\"\"\n",
    "            data += [best_X.tolist() + best_pred.tolist()]\n",
    "\n",
    "    print(np.count_nonzero(best_matched)/len(best_matched))\n",
    "    \n",
    "    all_best_ys = np.mean(all_best_ys, axis=0)\n",
    "    all_best_possible_ys = np.mean(all_best_possible_ys, axis=0)\n",
    "\n",
    "    \"\"\" Besides saving the data we can also check the accuracy of each positions assignment (only for narrow selection). \"\"\"\n",
    "\n",
    "    f = open(\"data/particle_assignment_training_data/particle_assignment_accuracy_narrow_selection_\" + ending + \".csv\", \"w\")\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(all_best_ys)\n",
    "    writer.writerow(all_best_possible_ys)\n",
    "    f.close()\n",
    "\n",
    "    \"\"\" Write to file. \"\"\"\n",
    "\n",
    "    f = open(\"data/mass_reco/mass_reco_input_narrow_selection_\" + ending + \".csv\", \"w\")\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(column_labels_mass_reco)\n",
    "    writer.writerows(data)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load wide selection ttH, ttZ data from files. \"\"\"\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/labels_dict_ttH.csv\") as f:\n",
    "    lines_ttH = f.readlines()\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/labels_dict_ttZ.csv\") as f:\n",
    "    lines_ttZ = f.readlines()\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/train_ids_ttH.csv\") as f:\n",
    "    train_ids_ttH = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/train_ids_ttZ.csv\") as f:\n",
    "    train_ids_ttZ = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/test_ids_ttH.csv\") as f:\n",
    "    test_ids_ttH = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/test_ids_ttZ.csv\") as f:\n",
    "    test_ids_ttZ = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/val_ids_ttH.csv\") as f:\n",
    "    val_ids_ttH = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/val_ids_ttZ.csv\") as f:\n",
    "    val_ids_ttZ = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "lines = np.concatenate((lines_ttH,lines_ttZ), axis = 0)\n",
    "\n",
    "labels_dict = {}\n",
    "\n",
    "for line in lines:\n",
    "    row = np.fromstring(line, dtype=float, sep=',')\n",
    "    key = str(int(row[0]))\n",
    "    value = (row[1:]).reshape((int(len(row[1:])/5),5))\n",
    "    labels_dict[key] = value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 04:15:00.459747: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-01-04 04:15:00.470594: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3100000000 Hz\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Same as for narrow selection, but this time the wide selection ttH and ttZ is used. \"\"\"\n",
    "\n",
    "ids_list = [train_ids_ttH, val_ids_ttH, test_ids_ttH, train_ids_ttZ, val_ids_ttZ, test_ids_ttZ]\n",
    "file_name_endings = [\"train_ttH\", \"val_ttH\",\"test_ttH\",\"train_ttZ\",\"val_ttZ\",\"test_ttZ\"]\n",
    "\n",
    "\n",
    "for ids,ending in zip(ids_list,file_name_endings): \n",
    "    predict_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=False, standardize=True, data_path = \"data/particle_assignment_data_to_be_processed_ttH_ttZ/data\")\n",
    "    write_to_file_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=True, standardize=False, data_path = \"data/particle_assignment_data_to_be_processed_ttH_ttZ/data\")\n",
    "\n",
    "    test_ones_diff = 0 \n",
    "    test_samples_count = 0\n",
    "\n",
    "    data = []\n",
    "    all_best_ys = []\n",
    "    all_best_possible_ys = []\n",
    "\n",
    "    y_pred_best = []\n",
    "    y_true_best = []\n",
    "    for i in range(len(predict_generator)):\n",
    "        X_test = predict_generator[i][0]\n",
    "        y_test = predict_generator[i][1]\n",
    "\n",
    "        X_to_file = write_to_file_generator[i][0]\n",
    "\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        X_y_preds = np.concatenate((X_to_file, y_test, preds),axis=1)\n",
    "        \n",
    "        ids = np.unique(X_to_file[:,0])\n",
    "\n",
    "        X_to_file = X_to_file[:,1:]\n",
    "\n",
    "        for id in ids:\n",
    "            X_y_preds_all_combinations = np.array([row[1:] for row in X_y_preds if row[0] == id])\n",
    "            X_all_combinations = X_y_preds_all_combinations[:,:X_to_file.shape[1]]\n",
    "            preds_all_combinations = X_y_preds_all_combinations[:,X_to_file.shape[1]+y_test.shape[1]:]\n",
    "\n",
    "            product = np.product(preds_all_combinations, axis=1)\n",
    "\n",
    "            best_pred = preds_all_combinations[np.argmax(product)]\n",
    "            best_X = X_all_combinations[np.argmax(product)]\n",
    "\n",
    "            data += [best_X.tolist() + best_pred.tolist()]\n",
    "\n",
    "    f = open(\"data/mass_reco/mass_reco_input_wide_selection_\" + ending + \".csv\", \"w\")\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(column_labels_mass_reco)\n",
    "    writer.writerows(data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load wide selection ttW, tt data from files. \"\"\"\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/labels_dict_ttW.csv\") as f:\n",
    "    lines_ttW = f.readlines()\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/labels_dict_tt.csv\") as f:\n",
    "    lines_tt = f.readlines()\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/train_ids_ttW.csv\") as f:\n",
    "    train_ids_ttW = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/train_ids_tt.csv\") as f:\n",
    "    train_ids_tt = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/test_ids_ttW.csv\") as f:\n",
    "    test_ids_ttW = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/test_ids_tt.csv\") as f:\n",
    "    test_ids_tt = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/val_ids_ttW.csv\") as f:\n",
    "    val_ids_ttW = np.loadtxt(f, delimiter=\",\")\n",
    "with open(\"data/particle_assignment_data_to_be_processed_ttW_tt/val_ids_tt.csv\") as f:\n",
    "    val_ids_tt = np.loadtxt(f, delimiter=\",\")\n",
    "\n",
    "lines = np.concatenate((lines_ttW,lines_tt), axis = 0)\n",
    "\n",
    "labels_dict = {}\n",
    "\n",
    "for line in lines:\n",
    "    row = np.fromstring(line, dtype=float, sep=',')\n",
    "    key = str(int(row[0]))\n",
    "    value = (row[1:]).reshape((int(len(row[1:])/5),5))\n",
    "    labels_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147/792696063.py:59: RuntimeWarning: divide by zero encountered in reciprocal\n",
      "  weights = np.reciprocal(X[:,-1])*200\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Same as for narrow selection, but this time the wide selection ttW and tt is used. \"\"\"\n",
    "\n",
    "ids_list = [train_ids_ttW, val_ids_ttW, test_ids_ttW, train_ids_tt, val_ids_tt, test_ids_tt]\n",
    "file_name_endings = [\"train_ttW\",\"val_ttW\",\"test_ttW\",\"train_tt\",\"val_tt\",\"test_tt\"]\n",
    "\n",
    "\n",
    "for ids,ending in zip(ids_list,file_name_endings): \n",
    "    predict_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=False, standardize=True, data_path=\"data/particle_assignment_data_to_be_processed_ttW_tt/data\")\n",
    "    write_to_file_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=True, standardize=False, data_path=\"data/particle_assignment_data_to_be_processed_ttW_tt/data\")\n",
    "\n",
    "    test_ones_diff = 0 \n",
    "    test_samples_count = 0\n",
    "\n",
    "    data = []\n",
    "    all_best_ys = []\n",
    "    all_best_possible_ys = []\n",
    "\n",
    "    y_pred_best = []\n",
    "    y_true_best = []\n",
    "    for i in range(len(predict_generator)):\n",
    "        X_test = predict_generator[i][0]\n",
    "        y_test = predict_generator[i][1]\n",
    "\n",
    "        X_to_file = write_to_file_generator[i][0]\n",
    "\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        X_y_preds = np.concatenate((X_to_file, y_test, preds),axis=1)\n",
    "        \n",
    "        ids = np.unique(X_to_file[:,0])\n",
    "\n",
    "        X_to_file = X_to_file[:,1:]\n",
    "\n",
    "        for id in ids:\n",
    "            X_y_preds_all_combinations = np.array([row[1:] for row in X_y_preds if row[0] == id])\n",
    "            X_all_combinations = X_y_preds_all_combinations[:,:X_to_file.shape[1]]\n",
    "            preds_all_combinations = X_y_preds_all_combinations[:,X_to_file.shape[1]+y_test.shape[1]:]\n",
    "\n",
    "            product = np.product(preds_all_combinations, axis=1)\n",
    "\n",
    "            best_pred = preds_all_combinations[np.argmax(product)]\n",
    "            best_X = X_all_combinations[np.argmax(product)]\n",
    "\n",
    "            data += [best_X.tolist() + best_pred.tolist()]\n",
    "\n",
    "    f = open(\"data/mass_reco/mass_reco_input_wide_selection_\" + ending + \".csv\", \"w\")\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(column_labels_mass_reco)\n",
    "    writer.writerows(data)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "faf4958e2dce26c676752487d7d4cf22fe757853dc487cc7f633ed9404dfd7e8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('conda_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
