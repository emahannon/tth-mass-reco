{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# coding: utf-8"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[1]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import csv\n", "import keras"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import keras.backend as K\n", "import math\n", "import matplotlib.pyplot as plt\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import tensorflow as tf"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import time\n", "import pandas as pd\n", "import seaborn as sns"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from keras import optimizers, metrics\n", "from keras.layers import Dense, LayerNormalization, BatchNormalization, Dropout, GaussianNoise\n", "from keras.models import load_model\n", "from constants import column_labels_mass_reco"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[2]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Modified from source: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly<br>\n", "Used to serve data from files to the neural network."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DataGenerator(keras.utils.Sequence):\n", "\t'Generates data for Keras'\n", "\tdef __init__(self, list_IDs, labels, batch_size=32, n_features=67, shuffle=True, data_path=\"\", scaler = \"../scaler_params/particle_assignment_scaler_params.csv\", write_to_file = False, standardize = True):\n", "\t\t'Initialization'\n", "\t\tself.n_features = n_features\n", "\t\tself.batch_size = batch_size\n", "\t\tself.labels = labels\n", "\t\tself.list_IDs = list_IDs\n", "\t\tself.shuffle = shuffle\n", "\t\tself.write_to_file = write_to_file\n", "\t\tself.standardize = standardize\n", "\t\tself.data_path = data_path\n", "\t\twith open(scaler) as f:\n", "\t\t\tscaler_params = np.loadtxt(f, delimiter=\",\")\n", "\t\t\tself.scaler = scaler_params\n", "\t\tself.on_epoch_end()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tdef __len__(self):\n", "\t\t'Denotes the number of batches per epoch'\n", "\t\treturn int(np.ceil(len(self.list_IDs) / self.batch_size))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tdef __getitem__(self, index):\n", "\t\t'Generate one batch of data'\n", "\t\t# Generate indexes of the batch\n", "\t\tindexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t# Find list of IDs\n", "\t\tlist_IDs_temp = [self.list_IDs[k] for k in indexes]\n", "\t\t# Generate data\n", "\t\tX, y, weights = self.__data_generation(list_IDs_temp)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\treturn X, y, weights"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tdef on_epoch_end(self):\n", "\t\t'Updates indexes after each epoch'\n", "\t\tself.indexes = np.arange(len(self.list_IDs))\n", "\t\tif self.shuffle == True:\n", "\t\t\tnp.random.shuffle(self.indexes)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tdef __data_generation(self, list_IDs_temp):\n", "\t\t'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n", "\t\t# Initialization\n", "\t\tX = np.zeros((1,self.n_features),dtype=float)\n", "\t\ty = np.zeros((1,5),dtype=int)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t# Generate data\n", "\t\tfor i, ID in enumerate(list_IDs_temp):\n", "\t\t\t# Store sample\n", "\t\t\t# print(X.shape)\n", "\t\t\t# print((np.load(self.data_path +'/'+ str(int(ID)) + '.npy').shape))\n", "\t\t\tX = np.concatenate((X,np.load(self.data_path +'/'+ str(int(ID)) + '.npy')),axis=0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\t# Store class\n", "\t\t\ty = np.concatenate((y,self.labels[str(int(ID))]),axis=0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tX = X[1:,:]\n", "\t\ty = y[1:,:]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tweights = np.reciprocal(X[:,-1])*200"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tif self.write_to_file:\n", "\t\t\tX = np.concatenate((X[:,:-5],X[:,-4:]), axis = 1)   # Exclude weights.\n", "\t\telse:\n", "\t\t\tX = X[:,1:-5]\n", "\t\t\tif self.standardize:\n", "\t\t\t\tX = (X-self.scaler[0])/self.scaler[1]     # Standardize"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\treturn X, y, weights"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tdef get_all(self):\n", "\t\tX = []\n", "\t\ty = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tfor i in range(self.__len__()):\n", "\t\t\tX_y = self.__getitem__(i)\n", "\t\t\tX += X_y[0].tolist()\n", "\t\t\ty += X_y[1].tolist()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tX = np.array(X)\n", "\t\ty = np.array(y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\treturn X, y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[3]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n Loss function. \n<br>\n", "# source: https://stackoverflow.com/questions/46009619/keras-weighted-binary-crossentropy<br>\n", "def create_weighted_binary_crossentropy(ones_weights,zeros_weights):<br>\n", "\tdef weighted_binary_crossentropy(y_true, y_pred):<br>\n", "\t\tb_ce = K.binary_crossentropy(y_true, y_pred)<br>\n", "\t\tweight_vector = y_true * ones_weights + (1. - y_true) * zeros_weights<br>\n", "\t\tweighted_b_ce = weight_vector * b_ce<br>\n", "\t\treturn K.mean(weighted_b_ce)<br>\n", "\treturn weighted_binary_crossentropy<br>\n", "# In[4]:<br>\n", "Metrics \n<br>\n", "source: https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def custom_f1(y_true, y_pred):\n", "\tdef recall_m(y_true, y_pred):\n", "\t\tTP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n", "\t\tPositives = K.sum(K.round(K.clip(y_true, 0, 1)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\trecall = TP / (Positives+K.epsilon())\n", "\t\treturn recall"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tdef precision_m(y_true, y_pred):\n", "\t\tTP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n", "\t\tPred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tprecision = TP / (Pred_Positives+K.epsilon())\n", "\t\treturn precision"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tprecision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\treturn 2*((precision*recall)/(precision+recall+K.epsilon()))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["source: https://stackoverflow.com/questions/39895742/matthews-correlation-coefficient-with-keras"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def matthews_correlation(y_true, y_pred):\n", "\ty_pred_pos = K.round(K.clip(y_pred, 0, 1))\n", "\ty_pred_neg = 1 - y_pred_pos"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\ty_pos = K.round(K.clip(y_true, 0, 1))\n", "\ty_neg = 1 - y_pos"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\ttp = K.sum(y_pos * y_pred_pos)\n", "\ttn = K.sum(y_neg * y_pred_neg)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tfp = K.sum(y_neg * y_pred_pos)\n", "\tfn = K.sum(y_pos * y_pred_neg)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tnumerator = (tp * tn - fp * fn)\n", "\tdenominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\treturn numerator / (denominator + K.epsilon())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["METRICS = [\n", "\t  keras.metrics.Precision(name='precision'),\n", "\t  keras.metrics.Recall(name='recall'),\n", "\t  keras.metrics.AUC(name='auc'),\n", "\t  keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n", "\t  matthews_correlation,\n", "\t  custom_f1,\n", "]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[5]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load the trained model."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = load_model(\"models/particle_assignment_model\", custom_objects={'custom_f1':custom_f1, 'matthews_correlation':matthews_correlation, 'weighted_binary_crossentropy':create_weighted_binary_crossentropy})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[6]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n Load narrow selection ttH, ttZ data from files. \n<br>\n", "with open(\"../data/particle_assignment_training_data/labels_dict_ttH.csv\") as f:<br>\n", "\tlines_ttH = f.readlines()<br>\n", "# with open(\"../data/particle_assignment_training_data/labels_dict_ttZ.csv\") as f:<br>\n", "# \tlines_ttZ = f.readlines()<br>\n", "with open(\"../data/particle_assignment_training_data/train_ids_ttH.csv\") as f:<br>\n", "\ttrain_ids_ttH = np.loadtxt(f, delimiter=\",\")<br>\n", "# with open(\"../data/particle_assignment_training_data/train_ids_ttZ.csv\") as f:<br>\n", "# \ttrain_ids_ttZ = np.loadtxt(f, delimiter=\",\")<br>\n", "with open(\"../data/particle_assignment_training_data/test_ids_ttH.csv\") as f:<br>\n", "\ttest_ids_ttH = np.loadtxt(f, delimiter=\",\")<br>\n", "# with open(\"../data/particle_assignment_training_data/test_ids_ttZ.csv\") as f:<br>\n", "# \ttest_ids_ttZ = np.loadtxt(f, delimiter=\",\")<br>\n", "with open(\"../data/particle_assignment_training_data/val_ids_ttH.csv\") as f:<br>\n", "\tval_ids_ttH = np.loadtxt(f, delimiter=\",\")<br>\n", "# with open(\"../data/particle_assignment_training_data/val_ids_ttZ.csv\") as f:<br>\n", "# \tval_ids_ttZ = np.loadtxt(f, delimiter=\",\")<br>\n", "# lines = np.concatenate((lines_ttH,lines_ttZ), axis = 0)<br>\n", "lines = lines_ttH<br>\n", "labels_dict = {}<br>\n", "for line in lines:<br>\n", "\trow = np.fromstring(line, dtype=float, sep=',')<br>\n", "\tkey = str(int(row[0]))<br>\n", "\tvalue = (row[1:]).reshape((int(len(row[1:])/5),5))<br>\n", "\tlabels_dict[key] = value<br>\n", "# In[11]:<br>\n", "The neural network produces probabilitise of each of the five positions being correctly assigned in a permutation of an event."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["The permutation with the largest product of the respective probabilities is chosen.\n", "The data of the chosen permutation is then saved to a file to be used in the mass reconstruction \"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Narrow ttH, ttZ."]}, {"cell_type": "markdown", "metadata": {}, "source": ["FIX THIS<br>\n", "ids_list = [train_ids_ttH, val_ids_ttH, test_ids_ttH, train_ids_ttZ, val_ids_ttZ, test_ids_ttZ]<br>\n", "file_name_endings = [\"train_ttH\",\"val_ttH\",\"test_ttH\",\"train_ttZ\",\"val_ttZ\",\"test_ttZ\"]<br>\n", "ids_list = [test_ids_ttH, test_ids_ttZ]<br>\n", "file_name_endings = [\"test_ttH\",\"test_ttZ\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ids_list = [train_ids_ttH, val_ids_ttH, test_ids_ttH]\n", "file_name_endings = [\"train_ttH\",\"val_ttH\",\"test_ttH\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for ids,ending in zip(ids_list,file_name_endings):\n", "\t\"\"\" One generator for obtaining the probability vectors. Second generator to obtain the precise data we want to save to the file. \"\"\"\n", "\tpredict_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=False, standardize=True, data_path = \"../data/particle_assignment_training_data/data\")\n", "\twrite_to_file_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=True, standardize=False, data_path = \"../data/particle_assignment_training_data/data\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\ttest_ones_diff = 0\n", "\ttest_samples_count = 0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tdata = []\n", "\tall_best_ys = []\n", "\tall_best_possible_ys = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\ty_pred_best = []\n", "\ty_true_best = []\n", "\tbest_matched = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tfor i in range(len(predict_generator)):\n", "\t\tX_test = predict_generator[i][0]\n", "\t\ty_test = predict_generator[i][1]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tX_to_file = write_to_file_generator[i][0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\t\t\n Get probability vectors. \n<br>\n", "\t\tpreds = model.predict(X_test)<br>\n", "\t\tX_y_preds = np.concatenate((X_to_file, y_test, preds),axis=1)<br>\n", "\t\tids = np.unique(X_to_file[:,0])<br>\n", "\t\tX_to_file = X_to_file[:,1:]<br>\n", "\n Each ID represents one event. \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tfor id in ids:\n", "\t\t\tX_y_preds_all_combinations = np.array([row[1:] for row in X_y_preds if row[0] == id])\n", "\t\t\tX_all_combinations = X_y_preds_all_combinations[:,:X_to_file.shape[1]]\n", "\t\t\ty_all_combinations = X_y_preds_all_combinations[:,X_to_file.shape[1]:X_to_file.shape[1]+y_test.shape[1]]\n", "\t\t\tpreds_all_combinations = X_y_preds_all_combinations[:,X_to_file.shape[1]+y_test.shape[1]:]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\tproduct = np.product(preds_all_combinations, axis=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\t\t\t\n Largest product yields the best (chosen) assignment. \n<br>\n", "\t\t\tbest_pred = preds_all_combinations[np.argmax(product)]<br>\n", "\t\t\tbest_y = y_all_combinations[np.argmax(product)]<br>\n", "\t\t\tbest_possible_y = y_all_combinations[np.argmax(np.sum(y_all_combinations, axis = 1))]<br>\n", "\t\t\tbest_matched.append(np.sum(best_y) >= np.sum(best_possible_y))<br>\n", "\t\t\tall_best_ys.append(best_y)<br>\n", "\t\t\tall_best_possible_ys.append(best_possible_y)<br>\n", "\t\t\tbest_X = X_all_combinations[np.argmax(product)]<br>\n", "\t\n Save data we will later write to file. \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\tdata += [best_X.tolist() + best_pred.tolist()]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tprint(np.count_nonzero(best_matched)/len(best_matched))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tall_best_ys = np.mean(all_best_ys, axis=0)\n", "\tall_best_possible_ys = np.mean(all_best_possible_ys, axis=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\t\n Besides saving the data we can also check the accuracy of each positions assignment (only for narrow selection). \n<br>\n", "\tf = open(\"../data/particle_assignment_training_data/particle_assignment_accuracy_narrow_selection_\" + ending + \".csv\", \"w\")<br>\n", "\twriter = csv.writer(f)<br>\n", "\twriter.writerow(all_best_ys)<br>\n", "\twriter.writerow(all_best_possible_ys)<br>\n", "\tf.close()<br>\n", " Write to file. \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tf = open(\"../data/mass_reco/mass_reco_input_narrow_selection_\" + ending + \".csv\", \"w\")\n", "\twriter = csv.writer(f)\n", "\twriter.writerow(column_labels_mass_reco)\n", "\twriter.writerows(data)\n", "\tf.close()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n Load wide selection ttH, ttZ data from files. \n<br>\n", "with open(\"../data/particle_assignment_data_to_be_processed_ttH_ttZ/labels_dict_ttH.csv\") as f:<br>\n", "\tlines_ttH = f.readlines()<br>\n", "# with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/labels_dict_ttZ.csv\") as f:<br>\n", "# \tlines_ttZ = f.readlines()<br>\n", "with open(\"../data/particle_assignment_data_to_be_processed_ttH_ttZ/train_ids_ttH.csv\") as f:<br>\n", "\ttrain_ids_ttH = np.loadtxt(f, delimiter=\",\")<br>\n", "# with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/train_ids_ttZ.csv\") as f:<br>\n", "# \ttrain_ids_ttZ = np.loadtxt(f, delimiter=\",\")<br>\n", "with open(\"../data/particle_assignment_data_to_be_processed_ttH_ttZ/test_ids_ttH.csv\") as f:<br>\n", "\ttest_ids_ttH = np.loadtxt(f, delimiter=\",\")<br>\n", "# with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/test_ids_ttZ.csv\") as f:<br>\n", "# \ttest_ids_ttZ = np.loadtxt(f, delimiter=\",\")<br>\n", "with open(\"../data/particle_assignment_data_to_be_processed_ttH_ttZ/val_ids_ttH.csv\") as f:<br>\n", "\tval_ids_ttH = np.loadtxt(f, delimiter=\",\")<br>\n", "# with open(\"data/particle_assignment_data_to_be_processed_ttH_ttZ/val_ids_ttZ.csv\") as f:<br>\n", "# \tval_ids_ttZ = np.loadtxt(f, delimiter=\",\")<br>\n", "#lines = np.concatenate((lines_ttH,lines_ttZ), axis = 0)<br>\n", "lines = lines_ttH<br>\n", "labels_dict = {}<br>\n", "for line in lines:<br>\n", "\trow = np.fromstring(line, dtype=float, sep=',')<br>\n", "\tkey = str(int(row[0]))<br>\n", "\tvalue = (row[1:]).reshape((int(len(row[1:])/5),5))<br>\n", "\tlabels_dict[key] = value<br>\n", "# In[ ]:<br>\n", "Same as for narrow selection, but this time the wide selection ttH and ttZ is used. \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ids_list = [train_ids_ttH, val_ids_ttH, test_ids_ttH, train_ids_ttZ, val_ids_ttZ, test_ids_ttZ]<br>\n", "file_name_endings = [\"train_ttH\", \"val_ttH\",\"test_ttH\",\"train_ttZ\",\"val_ttZ\",\"test_ttZ\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ids_list = [train_ids_ttH, val_ids_ttH, test_ids_ttH]\n", "file_name_endings = [\"train_ttH\", \"val_ttH\",\"test_ttH\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for ids,ending in zip(ids_list,file_name_endings):\n", "\tpredict_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=False, standardize=True, data_path = \"../data/particle_assignment_data_to_be_processed_ttH_ttZ/data\")\n", "\twrite_to_file_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=True, standardize=False, data_path = \"../data/particle_assignment_data_to_be_processed_ttH_ttZ/data\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\ttest_ones_diff = 0\n", "\ttest_samples_count = 0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tdata = []\n", "\tall_best_ys = []\n", "\tall_best_possible_ys = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\ty_pred_best = []\n", "\ty_true_best = []\n", "\tfor i in range(len(predict_generator)):\n", "\t\tX_test = predict_generator[i][0]\n", "\t\ty_test = predict_generator[i][1]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tX_to_file = write_to_file_generator[i][0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tpreds = model.predict(X_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tX_y_preds = np.concatenate((X_to_file, y_test, preds),axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tids = np.unique(X_to_file[:,0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tX_to_file = X_to_file[:,1:]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tfor id in ids:\n", "\t\t\tX_y_preds_all_combinations = np.array([row[1:] for row in X_y_preds if row[0] == id])\n", "\t\t\tX_all_combinations = X_y_preds_all_combinations[:,:X_to_file.shape[1]]\n", "\t\t\tpreds_all_combinations = X_y_preds_all_combinations[:,X_to_file.shape[1]+y_test.shape[1]:]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\tproduct = np.product(preds_all_combinations, axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\tbest_pred = preds_all_combinations[np.argmax(product)]\n", "\t\t\tbest_X = X_all_combinations[np.argmax(product)]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\tdata += [best_X.tolist() + best_pred.tolist()]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tf = open(\"../data/mass_reco/mass_reco_input_wide_selection_\" + ending + \".csv\", \"w\")\n", "\twriter = csv.writer(f)\n", "\twriter.writerow(column_labels_mass_reco)\n", "\twriter.writerows(data)\n", "\tf.close()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["COMMENTED OUT ALL ttW tt FROM HERE TO THE BOTTOM OF THE CODE<br>\n", "# \n Load wide selection ttW, tt data from files. \n<br>\n", "#<br>\n", "with open(\"../data/particle_assignment_data_to_be_processed_ttW_tt/labels_dict_ttW.csv\") as f:<br>\n", "\tlines_ttW = f.readlines()<br>\n", "# with open(\"../data/particle_assignment_data_to_be_processed_ttW_tt/labels_dict_tt.csv\") as f:<br>\n", "# \tlines_tt = f.readlines()<br>\n", "#<br>\n", "with open(\"../data/particle_assignment_data_to_be_processed_ttW_tt/train_ids_ttW.csv\") as f:<br>\n", "\ttrain_ids_ttW = np.loadtxt(f, delimiter=\",\")<br>\n", "# with open(\"../data/particle_assignment_data_to_be_processed_ttW_tt/train_ids_tt.csv\") as f:<br>\n", "# \ttrain_ids_tt = np.loadtxt(f, delimiter=\",\")<br>\n", "#<br>\n", "with open(\"../data/particle_assignment_data_to_be_processed_ttW_tt/test_ids_ttW.csv\") as f:<br>\n", "\ttest_ids_ttW = np.loadtxt(f, delimiter=\",\")<br>\n", "# with open(\"../data/particle_assignment_data_to_be_processed_ttW_tt/test_ids_tt.csv\") as f:<br>\n", "# \ttest_ids_tt = np.loadtxt(f, delimiter=\",\")<br>\n", "#<br>\n", "with open(\"../data/particle_assignment_data_to_be_processed_ttW_tt/val_ids_ttW.csv\") as f:<br>\n", "\tval_ids_ttW = np.loadtxt(f, delimiter=\",\")<br>\n", "# with open(\"../data/particle_assignment_data_to_be_processed_ttW_tt/val_ids_tt.csv\") as f:<br>\n", "# \tval_ids_tt = np.loadtxt(f, delimiter=\",\")<br>\n", "#<br>\n", "# lines = np.concatenate((lines_ttW,lines_tt), axis = 0)<br>\n", "lines = np.concatenate((lines_ttW), axis = 0)<br>\n", "#<br>\n", "labels_dict = {}<br>\n", "for line in lines:<br>\n", "\trow = np.fromstring(line, dtype=float, sep=',')<br>\n", "\tkey = str(int(row[0]))<br>\n", "\tvalue = (row[1:]).reshape((int(len(row[1:])/5),5))<br>\n", "\tlabels_dict[key] = value<br>\n", "#<br>\n", "#<br>\n", "# # In[ ]:<br>\n", "#<br>\n", "#<br>\n", "\n Same as for narrow selection, but this time the wide selection ttW and tt is used. \n<br>\n", "<br>\n", "ids_list = [train_ids_ttW, val_ids_ttW, test_ids_ttW, train_ids_tt, val_ids_tt, test_ids_tt]<br>\n", "file_name_endings = [\"train_ttW\",\"val_ttW\",\"test_ttW\",\"train_tt\",\"val_tt\",\"test_tt\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ids_list = [train_ids_ttW, val_ids_ttW, test_ids_ttW]\n", "file_name_endings = [\"train_ttW\",\"val_ttW\",\"test_ttW\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for ids,ending in zip(ids_list,file_name_endings):\n", "\tpredict_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=False, standardize=True, data_path=\"data/particle_assignment_data_to_be_processed_ttW_tt/data\")\n", "\twrite_to_file_generator = DataGenerator(ids, labels_dict, batch_size=128, shuffle=False, write_to_file=True, standardize=False, data_path=\"data/particle_assignment_data_to_be_processed_ttW_tt/data\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\ttest_ones_diff = 0\n", "\ttest_samples_count = 0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tdata = []\n", "\tall_best_ys = []\n", "\tall_best_possible_ys = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\ty_pred_best = []\n", "\ty_true_best = []\n", "\tfor i in range(len(predict_generator)):\n", "\t\tX_test = predict_generator[i][0]\n", "\t\ty_test = predict_generator[i][1]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tX_to_file = write_to_file_generator[i][0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tpreds = model.predict(X_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tX_y_preds = np.concatenate((X_to_file, y_test, preds),axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tids = np.unique(X_to_file[:,0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tX_to_file = X_to_file[:,1:]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tfor id in ids:\n", "\t\t\tX_y_preds_all_combinations = np.array([row[1:] for row in X_y_preds if row[0] == id])\n", "\t\t\tX_all_combinations = X_y_preds_all_combinations[:,:X_to_file.shape[1]]\n", "\t\t\tpreds_all_combinations = X_y_preds_all_combinations[:,X_to_file.shape[1]+y_test.shape[1]:]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\tproduct = np.product(preds_all_combinations, axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\tbest_pred = preds_all_combinations[np.argmax(product)]\n", "\t\t\tbest_X = X_all_combinations[np.argmax(product)]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\tdata += [best_X.tolist() + best_pred.tolist()]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tf = open(\"../data/mass_reco/mass_reco_input_wide_selection_\" + ending + \".csv\", \"w\")\n", "\twriter = csv.writer(f)\n", "\twriter.writerow(column_labels_mass_reco)\n", "\twriter.writerows(data)\n", "\tf.close()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}